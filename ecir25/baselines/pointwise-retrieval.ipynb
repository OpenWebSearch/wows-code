{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WOWS-Eval Retrieva Baseline\n",
    "\n",
    "This is a retrieval baseline to WOWS-EVAL that uses a retrieval model to assign the probability that an unknown document is relevant. The query is used as query which retrieves against all unknown documents that are to-be judged. The probability that a document is relevant is then the min-max normalized rank of an unknown document in the ranking of all unknown documents to the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install 'wows-eval>=0.0.6' python-terrier==0.10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load the Data\n",
    "\n",
    "Pointwise models have a query and a document as input and output the probability that the document is relevant into a field `probability_relevant`. For this retrieval baseline, we just take a PyTerrier retrieval model and use the min-max normalized retrieval rank as probability.\n",
    "\n",
    "In the following, we will process the pointwise smoke test dataset. Please modify the variable `DATASET_ID` to submit for other datasets. See [tira.io/datasets?query=wows-eval](https://archive.tira.io/datasets?query=wows-eval) for an complete overview of dataset identifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.0 has loaded Terrier 5.11 (built by craig.macdonald on 2025-01-13 21:29) and terrier-helper 0.0.8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "from tqdm import tqdm\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "\n",
    "from tira.rest_api_client import Client\n",
    "from wows_eval import evaluate as wows_evaluate\n",
    "import pandas as pd\n",
    "from jnius import autoclass\n",
    "import numpy as np\n",
    "\n",
    "# For measuring consumed resources (e.g., GPU, CPU, RAM, etc.)\n",
    "from tirex_tracker import tracking, ExportFormat\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Dataset IDs visible at https://archive.tira.io/datasets?query=wows-eval\n",
    "DATASET_ID = 'wows-eval/pointwise-smoke-test-20250128-training'\n",
    "#DATASET_ID = 'wows-eval/pairwise-20250309-test'\n",
    "\n",
    "tira = Client()\n",
    "input_data = tira.pd.inputs(DATASET_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>query</th>\n",
       "      <th>unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32d23068-7440-4891-9958-42325f98a604</td>\n",
       "      <td>who sings monk theme song</td>\n",
       "      <td>This is a reference to the minor controversy that brewed among Monk fans over the introduction of the new theme song It's A Jungle Out There written and performed by Randy Newman in the second season of Monk.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cde83146-ac3e-4bc5-a959-f2006ac7b8de</td>\n",
       "      <td>who sings monk theme song</td>\n",
       "      <td>Walker, Texas Ranger. Chuck Norris thought “Eyes of a Ranger” would be the perfect theme song for his new show Walker, Texas Ranger. He wanted his friend Randy Travis should sing it, but CBS had a different idea: The network suggested Norris sing the theme himself.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id                      query  \\\n",
       "0  32d23068-7440-4891-9958-42325f98a604  who sings monk theme song   \n",
       "1  cde83146-ac3e-4bc5-a959-f2006ac7b8de  who sings monk theme song   \n",
       "\n",
       "                                                                                                                                                                                                                                                                     unknown  \n",
       "0                                                           This is a reference to the minor controversy that brewed among Monk fans over the introduction of the new theme song It's A Jungle Out There written and performed by Randy Newman in the second season of Monk.  \n",
       "1  Walker, Texas Ranger. Chuck Norris thought “Eyes of a Ranger” would be the perfect theme song for his new show Walker, Texas Ranger. He wanted his friend Randy Travis should sing it, but CBS had a different idea: The network suggested Norris sing the theme himself.  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Implement the Approach\n",
    "\n",
    "We wrap all computations into a [tirex_tracker.tracking](https://github.com/tira-io/tirex-tracker/) environment to measure the resources consumed for our computations and also a snapshot of our code in the [ir-metadata format](https://www.ir-metadata.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PCM Info: setrlimit for file limit 1000000 failed with error Operation not permitted\n",
      "\n",
      "=====  Processor information  =====\n",
      "Linux arch_perfmon flag  : yes\n",
      "Hybrid processor         : yes\n",
      "IBRS and IBPB supported  : yes\n",
      "STIBP supported          : yes\n",
      "Spec arch caps supported : yes\n",
      "Max CPUID level          : 32\n",
      "CPU model number         : 154\n",
      "ERROR: Can not open /sys/module/msr/parameters/allow_writes file.\n",
      "PCM Error: can't open MSR handle for core 0 (No such file or directory)\n",
      "Try no-MSR mode by setting env variable PCM_NO_MSR=1\n",
      "Can not access CPUs Model Specific Registers (MSRs).\n",
      "execute 'modprobe msr' as root user, then execute pcm as root user.\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.51it/s]\n"
     ]
    }
   ],
   "source": [
    "WMODEL = \"BM25\"\n",
    "system_name = f'pointwise-retrieval-{WMODEL}'.lower()\n",
    "\n",
    "def retrieve_and_normalize(query, retrieval_system, unk):    \n",
    "    ret = {}\n",
    "    tokeniser = autoclass(\"org.terrier.indexing.tokenisation.Tokeniser\").getTokeniser()\n",
    "    query_text = \" \".join(tokeniser.getTokens(query))\n",
    "    run = retrieval_system.search(query_text)\n",
    "    last_rank = -1\n",
    "    scores = {}\n",
    "    for _, i in run.iterrows():\n",
    "        assert last_rank < i['rank']\n",
    "        last_rank = i['rank']\n",
    "        if i['docno'] in unk:\n",
    "            scores[unk[i['docno']]] = i['rank']\n",
    "\n",
    "    max_score = max(scores.values())\n",
    "    min_score = min(scores.values())\n",
    "\n",
    "    return {k: ((v-min_score)/(max_score-min_score)) for k, v in scores.items()}\n",
    "\n",
    "!rm -Rf run\n",
    "with tracking(export_file_path='run/.metadata.yml', export_format=ExportFormat.IR_METADATA) as tracked:\n",
    "    queries = set(input_data['query'].unique())\n",
    "\n",
    "    def unknown_documents(query):\n",
    "        docs = set(input_data[input_data['query'] == query]['unknown'].unique())\n",
    "        return {f'{i[0]}-unkn': i[1] for i in zip(range(len(docs)), docs)}\n",
    "\n",
    "    results = {}\n",
    "    for query in tqdm(queries):\n",
    "        unk = unknown_documents(query)\n",
    "\n",
    "        docs = [{'docno': k, 'text': v} for k, v in unk.items()]\n",
    "        indexer = pt.IterDictIndexer(\"/tmp/index\", overwrite=True, meta={'docno': 100, 'text': 20480})\n",
    "        index_ref = indexer.index(docs)\n",
    "        retriever = pt.BatchRetrieve(index_ref, wmodel=WMODEL)\n",
    "        results[query] = retrieve_and_normalize(query, retriever, unk)\n",
    "\n",
    "    predictions = []\n",
    "    for _, i in input_data.iterrows():\n",
    "        res = results[i['query']]\n",
    "        predictions.append({\n",
    "            'id': i['id'],\n",
    "            'probability_relevant': res.get(i['unknown'], -1)\n",
    "        })\n",
    "    predictions = pd.DataFrame(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate and Submit Your Run\n",
    "\n",
    "We use the `wows_evaluate` method imported above to evaluate our predictions and to upload them, to TIRA.\n",
    "\n",
    "The `wows_evaluate` method has optional parameters that you can pass to describe your system and to include the resource measurements used during your computations in the ir-metadata format into your submission. You can remove those attributes or modify them for your submission accordingly. Call `help(wows_evaluate)` to see a full description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run uploaded to TIRA. Claim ownership via: https://www.tira.io/claim-submission/18f7f8aa-192f-4d08-a03b-0866d540ccae\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>system</th>\n",
       "      <th>tau_ap</th>\n",
       "      <th>kendall</th>\n",
       "      <th>spearman</th>\n",
       "      <th>pearson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pointwise-retrieval-bm25</td>\n",
       "      <td>0.040556</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     system    tau_ap   kendall  spearman  pearson\n",
       "0  pointwise-retrieval-bm25  0.040556  0.085714       0.1      0.1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wows_evaluate(\n",
    "    predictions,\n",
    "    DATASET_ID,\n",
    "    tracking_results=tracked,\n",
    "    upload=True,\n",
    "    system_name=system_name,\n",
    "    system_description=f'We use the PyTerrier retrieval model {WMODEL} to assign the probability that an unknown document is relevant. We rank all unknown documents that are to-be judged against the query. The probability that a document is relevant is then the min-max normalized rank of an unknown document in the ranking of all unknown documents to the query.'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
