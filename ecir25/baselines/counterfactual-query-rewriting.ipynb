{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyterrier as pt\n",
    "from tqdm import tqdm\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "\n",
    "from tira.rest_api_client import Client\n",
    "from wows_eval import evaluate as wows_evaluate\n",
    "import pandas as pd\n",
    "from jnius import autoclass\n",
    "import numpy as np\n",
    "\n",
    "# For measuring consumed resources (e.g., GPU, CPU, RAM, etc.)\n",
    "from tirex_tracker import tracking, ExportFormat\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "DATASET_ID = 'wows-eval/pairwise-smoke-test-20250210-training'\n",
    "#DATASET_ID = 'wows-eval/pairwise-20250309-test'\n",
    "\n",
    "tira = Client()\n",
    "input_data = tira.pd.inputs(DATASET_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryByRelevantDocument:\n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "    def process(self, query, retrieval_system, rel, unk):\n",
    "        if query in self.results:\n",
    "            raise ValueError('This query was already processed: ' + query)\n",
    "        \n",
    "        ret = {}\n",
    "        tokeniser = autoclass(\"org.terrier.indexing.tokenisation.Tokeniser\").getTokeniser()\n",
    "        for doc in rel.values():\n",
    "            doc_text = \" \".join(tokeniser.getTokens(doc))\n",
    "            run = retrieval_system.search(doc_text)\n",
    "            last_rank = -1\n",
    "            scores = {}\n",
    "            for _, i in run.iterrows():\n",
    "                assert last_rank < i['rank']\n",
    "                last_rank = i['rank']\n",
    "                if i['docno'] in unk:\n",
    "                    scores[unk[i['docno']]] = i['rank']\n",
    "\n",
    "            max_score = max(scores.values())\n",
    "            min_score = min(scores.values())\n",
    "            ret[doc] = {k: ((v-min_score)/(max_score-min_score)) for k, v in scores.items()}\n",
    "        \n",
    "        self.results[query] = ret\n",
    "\n",
    "\n",
    "class QueryByUnknownDocument:\n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "    def process(self, query, retrieval_system, rel, unk):\n",
    "        if query in self.results:\n",
    "            raise ValueError('This query was already processed: ' + query)\n",
    "        \n",
    "        ret = {}\n",
    "        tokeniser = autoclass(\"org.terrier.indexing.tokenisation.Tokeniser\").getTokeniser()\n",
    "        for doc in unk.values():\n",
    "            doc_text = \" \".join(tokeniser.getTokens(doc))\n",
    "            run = retrieval_system.search(doc_text)\n",
    "            last_rank = -1\n",
    "            dcg = 0\n",
    "            for _, i in run.iterrows():\n",
    "                assert last_rank < i['rank']\n",
    "                last_rank = i['rank']\n",
    "                if i['rank'] >= 20:\n",
    "                    break\n",
    "                if i['docno'] in rel:\n",
    "                    # https://github.com/joaopalotti/trectools/blob/master/trectools/trec_eval.py#L499C28-L499C56\n",
    "                    dcg += 1. / np.log2(i['rank']+1)\n",
    "            \n",
    "            ret[doc] = dcg\n",
    "        max_score = max(ret.values())\n",
    "        min_score = min(ret.values())\n",
    "        ret = {k: ((v-min_score)/(max_score-min_score)) for k, v in ret.items()}\n",
    "        ret = {r: ret for r in rel.values()}\n",
    "\n",
    "        self.results[query] = ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PCM Info: setrlimit for file limit 1000000 failed with error Operation not permitted\n",
      "\n",
      "=====  Processor information  =====\n",
      "Linux arch_perfmon flag  : yes\n",
      "Hybrid processor         : yes\n",
      "IBRS and IBPB supported  : yes\n",
      "STIBP supported          : yes\n",
      "Spec arch caps supported : yes\n",
      "Max CPUID level          : 32\n",
      "CPU model number         : 154\n",
      "ERROR: Can not open /sys/module/msr/parameters/allow_writes file.\n",
      "PCM Error: can't open MSR handle for core 0 (No such file or directory)\n",
      "Try no-MSR mode by setting env variable PCM_NO_MSR=1\n",
      "Can not access CPUs Model Specific Registers (MSRs).\n",
      "execute 'modprobe msr' as root user, then execute pcm as root user.\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.34it/s]\n"
     ]
    }
   ],
   "source": [
    "system_name = 'query-by-relevant-doc'\n",
    "#system_name = 'query-by-unknown-doc'\n",
    "\n",
    "!rm -Rf tmp\n",
    "with tracking(export_file_path='tmp/.metadata.yml', export_format=ExportFormat.IR_METADATA) as tracked_experiment:\n",
    "    queries = set(input_data['query'].unique())\n",
    "\n",
    "    def known_relevant_documents(query):\n",
    "        docs = set(input_data[input_data['query'] == query]['relevant'].unique())\n",
    "        return {f'{i[0]}-rel': i[1] for i in zip(range(len(docs)), docs)}\n",
    "\n",
    "    def unknown_documents(query):\n",
    "        docs = set(input_data[input_data['query'] == query]['unknown'].unique())\n",
    "        return {f'{i[0]}-unkn': i[1] for i in zip(range(len(docs)), docs)}\n",
    "\n",
    "    if system_name == 'query-by-relevant-doc':\n",
    "        processor = QueryByRelevantDocument()\n",
    "    elif system_name == 'query-by-unknown-doc':\n",
    "        processor = QueryByUnknownDocument()\n",
    "    else:\n",
    "        raise ValueError('foo')\n",
    "\n",
    "    for query in tqdm(queries):\n",
    "        rel = known_relevant_documents(query)\n",
    "        unk = unknown_documents(query)\n",
    "\n",
    "        docs = [{'docno': k, 'text': v} for k, v in rel.items()]+[{'docno': k, 'text': v} for k, v in unk.items()]\n",
    "        indexer = pt.IterDictIndexer(\"/tmp/index\", overwrite=True, meta={'docno': 100, 'text': 20480})\n",
    "        index_ref = indexer.index(docs)\n",
    "        bm25 = pt.BatchRetrieve(index_ref, wmodel='BM25')\n",
    "        processor.process(query, bm25, rel, unk)\n",
    "\n",
    "    predictions = []\n",
    "    for _, i in input_data.iterrows():\n",
    "        res = processor.results[i['query']]\n",
    "        res = res[i['relevant']]\n",
    "        predictions.append({\n",
    "            'id': i['id'],\n",
    "            'probability_relevant': res.get(i['unknown'], -1)\n",
    "        })\n",
    "    predictions = pd.DataFrame(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>system</th>\n",
       "      <th>tau_ap</th>\n",
       "      <th>kendall</th>\n",
       "      <th>spearman</th>\n",
       "      <th>pearson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>query-by-relevant-doc</td>\n",
       "      <td>0.526667</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>0.621429</td>\n",
       "      <td>0.621429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  system    tau_ap   kendall  spearman   pearson\n",
       "0  query-by-relevant-doc  0.526667  0.485714  0.621429  0.621429"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wows_evaluate(\n",
    "    predictions,\n",
    "    DATASET_ID,\n",
    "    environment=tracked_experiment,\n",
    "    system_name=system_name,\n",
    "    #upload=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
